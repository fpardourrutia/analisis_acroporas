---
title: "Corales Lorenzo"
author: "Mau, Fer"
date: "11 de noviembre de 2015"
output: html_document
---

##Panorama General

+ Se realizó un estudio previo mediante SIG para identificar las áreas donde es
factible encontrar poblaciones de A. palmata.

+ Luego se exploraron dichas áreas, y se seleccionaron los parches que en
realidad tienen cobertura de acrópora para ser muestreados. Éstos se digitalizaron.

+ Utilizando la digitalización de los parches, se obtuvo una muestra aleatoria de
sitios a visitar. La cantidad de sitios (r = 2m) en cada parche se seleccionó de
manera que cubrieran el 10% del mismo. De esta manera, todos los puntos deben caer
exactamente en cada parche del shape (al menos teóricamente.)

+ Por tanto, los sitios/polígonos muestreados no son representativos del arrecife
en que se encuentran, sin embargo, componen en un muestreo _estratificado_ de los
parches con _Acrópora sp._ dentro de cada arrecife.

+ Se observa que los puntos de GPS muchas veces no se encuentran dentro del parche
al que se refieren, y además se sabe que el GPS tiene un error (mayor en agua que
en tierra), por lo que la primera cuestión a resolver es cómo asignar variables
ambientales (de rasters con resolución espacial de 2m) a localizaciones
geográficas aproximadas.

+ Considerar que también hay error humano.

## Primera idea

1. Sacar un búfer (ó ventana, más natural para rasters) de 5m (o 3px, experiencia
de Mau) alrededor de cada punto del GPS, obtenemos los pixeles en la intersección
de ese búfer con algún parche, y sacamos agregados de cada variable.

2. Se elige la mediana porque es más robusta ante atípicos que la media, pero para
saber esto con precisión, conviene hacer una simulación: círculos con radio 1m
aparte, 2m aparte, etc, y qué porcentaje de los datos comparten.

3. Hacer pruebas con un GPS en el mar, para saber el error de medición. Sacar el
histograma de errores y obtener un tamaño de búfer apropiado.

## Segunda idea:

1. Segmentar la imagen y usar los superpixeles que coinciden con los puntos para
agregar los pixeles individuales.

Desventaja: no se está arreglando el error si el el punto realmente muestreado
cayó en otro superpixel.

## Tercera idea:

1. Segmentar la imagen.

2. Tomar un búfer más pequeño.

3. Usar las medias de los superpixeles en cada búfer para obtener el valor de
las variables ambientales que se van a asignar al pixel.

Desventaja: como el búfer es pequeño, podría no haber intersección entre los puntos
muestreados realmente y los del GPS (con errores).

```{r, message = FALSE}
library("raster")

# Para leer shapes
library("rgdal")

# Para trabajar con shapes.
#library("rgeos") # es una dependencia de raster

library("plyr")
library("dplyr")
library("tidyr")
library("readr")
library("ggplot2")
library("purrr")
library("stringi")

# Para calcular medianas ponderadas
library("matrixStats")

# Para usar escalamiento multidimensional con TSNE
#library("tsne")

# Para ajustar RandomForest
library("randomForest")

# Para procesar en paralelo la función "validacion_cruzada_por_segmentos"
library("doMC")
# Registrando número de núcleos
registerDoMC(4)

# Declarando la función para realizar la validación cruzada por segmentos
# df: un data frame con la información de las variables de entrada, la
# de salida, y la variable de segmentación con la que se realizará la
# validación cruzada por segmentos (folds).
# nombre_variable_secgmentacion: un string con el nombre de dicha variable.
# formula_prediccion: un objeto tipo fórmula, por ejemplo: y ~ x1 + x2,
# donde y es la variable de salida y x'i son las variables de entrada.
# num_iteraciones: número de veces que se repetirá la validación cruzada
# (cada etapa comprende el número de folds)

# Por ahora se utiliza randomForest y el error se mide como proporción de datos
# mal clasificados (sólo implementada para clasificación). Se está suponiendo
# que cada segmento tiene la misma cantidad de observaciones, por facilidad.

validacion_cruzada_por_segmentos <- function(df, nombre_variable_segmentacion, formula_prediccion, num_iteraciones){
  
  # Función que permite calcular el error de predicción al entrenar un RandomForest
  # especificado con una fórmula sobre un conjunto de entrenamiento y prediciendo
  # sobre uno de prueba.
  # La función regresa un df con el número de elementos en el set de prueba, así
  # como las proporciones de error y éxito de predicción.
  calcula_error_prediccion <- function(set_entrenamiento, set_prueba,
    formula_prediccion){
    
    # Ajustando modelo predictivo con los datos de entrenamiento:
    rf <- randomForest(formula_prediccion, set_entrenamiento)
    
    # Obteniendo los valores de predicción para cada elemento en el set de
    # prueba
    predicciones_prueba <- predict(rf, set_prueba, type = "class")
    
    # Obteniendo variable de salida de la fórmula
    y <- all.vars(formula_prediccion)[1]
    
    # Obteniendo el error de predicción para el set de prueba
    error_prediccion <- mean(predicciones_prueba != set_prueba[[y]])
    
    # Formando el resultado
    resultado <- data_frame(
      n = nrow(set_prueba),
      error_prediccion = error_prediccion,
      exito_prediccion = 1 - error_prediccion
    )
    
    return(resultado)
  }
  
  ## Creando lista con sets de entrenamiento y prueba a partir del "df" y el
  ## "nombre_variable_segmentación". Esto por eficiencia, para no crearlas en cada
  ## iteración
  
  # Enlistando los distitos segmentos de "df" de acuerdo a "nombre_variable_segmentacion"
  segmentos <- df[[nombre_variable_segmentacion]] %>%
    unique() %>%
    sort()
  
  # Creando lista con conjuntos de entrenamiento y de prueba.
  lista_sets_entrenamiento_prueba <- llply(segmentos, function(segmento, df, nombre_variable_segmentacion){
    
    # Creando conjunto de entrenamiento
    condicion_filtro_entrenamiento <- paste0(nombre_variable_segmentacion,
      " != \"", segmento, "\"")
    set_entrenamiento <- df %>%
      filter_(condicion_filtro_entrenamiento)
    
    # Creando conjunto de prueba
    condicion_filtro_prueba <- paste0(nombre_variable_segmentacion,
      " == \"", segmento, "\"")
    set_prueba <- df %>%
      filter_(condicion_filtro_prueba)
    
    lista_sets <- list(
      set_entrenamiento = set_entrenamiento,
      set_prueba = set_prueba
    )
      
    return(lista_sets)
  }, df, nombre_variable_segmentacion)
  
  # Nombrando los elementos de "lista_sets_entrenamiento_prueba" como los segmentos asociados a ellos:
  names(lista_sets_entrenamiento_prueba) <- as.character(segmentos)
  
  # Calculando los errores por segmento para todas las iteraciones de validación cruzada
  errores <- ldply(1:num_iteraciones, function(i, lista_sets_entrenamiento_prueba, formula_prediccion){
    
    #  Llamando la función "calcula_error_prediccion" para todos los elementos en "lista_sets_entrenamiento_prueba".
    errores_segmentos <- ldply(lista_sets_entrenamiento_prueba, function(lista_sets, formula_prediccion){
      error_segmento <- calcula_error_prediccion(lista_sets$set_entrenamiento,
        lista_sets$set_prueba, formula_prediccion)
      
      return(error_segmento)
      
    }, formula_prediccion)
    
    # Generando el resultado correspondiente a la i-ésima iteración
    resultado <- errores_segmentos %>%
      mutate(
        num_iteracion = i
        ) %>%
      select(
        num_iteracion,
        segmento = .id,
        everything()
      )
    
    return(resultado)
  }, lista_sets_entrenamiento_prueba, formula_prediccion, .parallel = TRUE)
}
  
```

Homologando coordenadas y projecciones de todos los rasters para crear un brick
de insumos:

```{r, warning=FALSE, message=FALSE}

# ../ es para regresarse una carpeta del working directory.
dir_datos <- "../../datos_analisis_detallado/"

# # Leyendo raster con las reflectancias del 2010 y del 2011 de manera independiente (imagen satelital)
# brick_reflectancias_2010 <- paste0(dir_datos, "reflectancias_2010/WV2_2010-12-20_Mexico_bref_geo_UTM16N.img") %>%
#   brick()
# brick_reflectancias_2011 <- paste0(dir_datos, "reflectancias_2011/WV2_2011-01-19_Mexico_bref_geo_UTM16N.img") %>%
#   brick()
# 
# # Leyendo los otros rasters:
# raster_batimetria <- paste0(dir_datos, "relieve/batimetria_real_pm_completa1.tif") %>%
#   raster()
# # pendiente: pendiente de máximo cambio de la batimetría
# raster_pendiente <- paste0(dir_datos, "relieve/slope_pm1.tif") %>%
#   raster()
# # aspecto: ángulo del gradiente de la batimetría
# raster_aspecto <- paste0(dir_datos, "relieve/aspect_pm.tif") %>%
#   raster()
# 
# # Con el siguiente comando, revisando las proyecciones, podemos ver que todas son iguales.
# #projection(raster_aspecto) == projection(raster_batimetria)
# 
# # Cortando los rasters del mismo tamaño para hacer un único ladrillo, para cada stack de reflectancias:
# # Todo se reproyectará a la proyección de "raster_pendiente", que engloba a ambos pero es ligeramente menor
# # en extensión que "raster_batimetria" (puesto que son pendientes calculadas.)
# 
# # La extensión de los stacks es la misma que la de "raster_pendiente"
# brick_reflectancias_2010_corregido <- resample(brick_reflectancias_2010, raster_pendiente, "bilinear")
# #extent(stack_reflectancias_2010_corregido) == extent(raster_pendiente)
# 
# brick_reflectancias_2011_corregido <- resample(brick_reflectancias_2011, raster_pendiente, "bilinear")
# #extent(stack_reflectancias_2011_corregido) == extent(raster_pendiente)
# 
# # Recortando "raster_batimetria"
# #projection(raster_batimetria) == projection(raster_pendiente)
# raster_batimetria_recortado <- crop(raster_batimetria, raster_pendiente)
# #extent(raster_batimetria_recortado) == extent(raster_pendiente)
# 
# # Recortando "raster_aspecto"
# #projection(raster_aspecto) == projection(raster_pendiente)
# raster_aspecto_recortado <- crop(raster_aspecto, raster_pendiente)
# #extent(raster_aspecto_recortado) == extent(raster_pendiente)
# 
# # Creando un nuevo brick con las imágenes satelitales del 2010, del 2011 y productos de relieve homologados
# # El brick almacena los datos de una manera más eficiente para el análisis que el stack.
# brick_insumos <- list(
#   subset(brick_reflectancias_2010_corregido, 1:4) %>%
#     unstack(),
#   subset(brick_reflectancias_2011_corregido, 1:4) %>%
#     unstack(),
#   list(
#     raster_batimetria_recortado,
#     raster_pendiente,
#     raster_aspecto_recortado
#     )
#   ) %>%
#   # Simplificar listas
#   flatten() %>%
#   brick()
# 
# # Arreglando los nombres de "brick_insumos"
# names(brick_insumos) <- c(
#   names(brick_reflectancias_2010_corregido)[1:4],
#   names(brick_reflectancias_2011_corregido)[1:4],
#   names(raster_batimetria_recortado),
#   names(raster_pendiente),
#   names(raster_aspecto_recortado)
# )
# 
# writeRaster(brick_insumos, "../../productos/brick_insumos.tif")
# 
# # Los nombres no se escriben en el brick, entonces los guardamos por su cuenta
# saveRDS(names(brick_insumos), "../../productos/names_brick_insumos.RData")

# Leyendo raster de insumos:
brick_insumos <- brick("../../productos/brick_insumos.tif")
names_brick_insumos <- readRDS("../../productos/names_brick_insumos.RData")
names(brick_insumos) <- names_brick_insumos

quartz()
plot(brick_insumos)

# Utilizar la intersección para homologar la imágenes del 2010 con la del 2011
# sería un problema de regresión:
# Si A es una matriz de pixeles por bandas (n x 4) para el 2010 y B lo mismo para
# el 2011, lo que se tendría que hacer es encontrar una matriz C (4x$) tal que
# A + AC = B, o AC = B-A, ésto es un problema de regresión.
```

Obteniendo shapes de trabajo (buffers de 5m alrededor de cada punto del GPS) y
shape de parches corregidos por Mau.

```{r, warning=FALSE, message=FALSE}
# Shape con buffers alrededor de cada punto del GPS, correspondientes a un sitio
# estimado de muestreo de acróporas.
sitios_estimados_acropora_sf <- readOGR(dsn = paste0(dir_datos, "2014_sitios_estimados_acropora"))
plot(sitios_estimados_acropora_sf[1:10,])

# Obteniendo el shape de parches de acrópora ajustados para intersectarlo con los con sitios estimados, y de esta manera, corregir parcialmente el error de GPS.
parches_acropora_ajustados_sh <- readOGR(dsn = paste0(dir_datos, "2014_parches_acropora_ajustados"))
#plot(parches_acropora_ajustados_sh[1:5,])
```

Intersectando ambos shapes:

```{r, warning=FALSE, message=FALSE}
# Obteniendo la intersección de ambos shapefiles.

# Comprobando proyecciones:
projection(parches_acropora_ajustados_sh) == projection(sitios_estimados_acropora_sf)
# Una es en UTM y otra en longlat, nos quedamos con UTM porque es la del brick.

sitios_estimados_acropora_sf_utm <- spTransform(sitios_estimados_acropora_sf, projection(parches_acropora_ajustados_sh))
projection(parches_acropora_ajustados_sh) == projection(sitios_estimados_acropora_sf_utm)

# Intersectando ambos shapes, byid = TRUE hace que no se haga un polígono gigante con
# todas las intersecciones, sino varios polígonos pequeños
# raster::intersect difiere de rgeos::gintersect en que preserva data.
intersecciones_parches_sitios_estimados <- raster::intersect(parches_acropora_ajustados_sh, sitios_estimados_acropora_sf_utm)

# Ploteando parches, primero se restringe el extent (primer shape) y luego se plotean
# de los otro shapes los objetos que quepan en dicho extent.

plot(extent(intersecciones_parches_sitios_estimados[1:5,]))
plot(intersecciones_parches_sitios_estimados, col = 'red', add = TRUE)
plot(sitios_estimados_acropora_sf_utm, add = TRUE)
plot(parches_acropora_ajustados_sh, add = TRUE)
# Tudo bem

# Es más eficiente extraer del brick primero lo correspondiente a los parches y
# reextraer de los buffers, porque se evita el problema combinatorio de parches
# vs buffers.
```

Extrayendo de cada punto las variables correspondientes a los pixeles en cada
elemento en el shape de buffers intersección shape de parches corregidos, y
formando un data frame con ellas.

```{r, warning=FALSE, message=FALSE}
# # Extrayendo del brick las variables correspondientes cada elemento en el shape
# # de intersecciones, por pixel.
# variables_raster_intersecciones_df <- raster::extract(brick_insumos,
#   intersecciones_parches_sitios_estimados,
#   method = "simple", cellnumbers = TRUE, df = TRUE, weights = TRUE)
# glimpse(variables_raster_intersecciones_df)
# 
# # weights es una variable que a cada pixel le asocia la proporción que le corresponde
# # del polígono considerado, por lo que siempre suman 1 por polígono.
# 
# # Algunos resúmenes:
# variables_raster_intersecciones_df %>%
#   group_by(ID) %>%
#   tally() %>%
#   View()
# 
# variables_raster_intersecciones_df %>%
#   group_by(ID) %>%
#   summarise(
#     uno = sum(weight)
#   ) %>%
#   View()
# 
# # Extrayendo para 2 intersecciones y comparando:
# raster::extract(brick_insumos,
#   intersecciones_parches_sitios_estimados[c(1,125),],
#   method = "simple", cellnumbers = TRUE, df = TRUE, weights = TRUE) %>%
#   View()
# # Todo bien, salen en orden.
#
# Haciendo el join de "variables_raster_intersecciones_df" con los datos de
# "intersecciones_parches_sitios_estimados" y extrayendo atributos necesarios:

# df_pixeles_desglosados <- variables_raster_intersecciones_df %>%
#   inner_join(intersecciones_parches_sitios_estimados@data %>%
#       mutate(
#         ID = 1:nrow(.)
#       ), by = "ID")
# glimpse(df_pixeles_desglosados)
#saveRDS(df_pixeles_desglosados, "../../productos/df_pixeles_desglosados.RData")

df_pixeles_desglosados <-readRDS("../../productos/df_pixeles_desglosados.RData")

# Viendo que cada ID corresponda a un Punto
df_pixeles_desglosados %>%
  group_by(ID, Punto) %>%
  tally() %>%
  nrow()

df_pixeles_desglosados$Punto %>%
  unique() %>%
  length()
# Perfecto!

# Revisando que cada pixel tenga al menos variables satelitales del 2010 o del 2011:
df_pixeles_desglosados %>%
  filter(
    !is.na(WV2_2010.12.20_Mexico_bref_geo_UTM16N.1) |
      !is.na(WV2_2011.01.19_Mexico_bref_geo_UTM16N.1)
  ) %>%
  nrow()

nrow(df_pixeles_desglosados)
# Perfecto
```

Calculando, para cada elemento en el shape de intersecciones, los agregados
(medias, medianas, desviaciones estándar) de cada una de las variables de
percepción remota (PR). Julián sugiere que para polígonos más grandes, se podrían
incluso utilizar cuantiles. Con ello, formar el data frame de trabajo.

Por el momento se excluyeron del análisis los puntos
que requieren de las imágenes satelitales del 2011, ya que son muy pocos (45 de
925), y requerirían un amplio procesamiento para integrarlos en este análisis.
```{r, warning=FALSE, message=FALSE}
# Creando el DF de trabajo para el 2010
# df_trabajo_2010 <- df_pixeles_desglosados %>%
#   filter(!is.na(WV2_2010.12.20_Mexico_bref_geo_UTM16N.1)) %>%
#   select(
#     ID,
#     WV2_2010_1 = WV2_2010.12.20_Mexico_bref_geo_UTM16N.1,
#     WV2_2010_2 = WV2_2010.12.20_Mexico_bref_geo_UTM16N.2,
#     WV2_2010_3 = WV2_2010.12.20_Mexico_bref_geo_UTM16N.3,
#     WV2_2010_4 = WV2_2010.12.20_Mexico_bref_geo_UTM16N.4,
#     batimetria = batimetria_real_pm_completa1,
#     pendiente = slope_pm1,
#     aspecto = aspect_pm,
#     peso = weight,
#     punto = Punto,
#     parche = Parche,
#     arrecife = Arrecife,
#     latitud = Lat,
#     longitud = Long,
#     profundidad_media_m = Av_depth_m,
#     cobertura_acropora = Acropora_c
#   ) %>%
#   mutate(
#     punto = as.character(punto) %>%
#       tolower(),
#     parche = as.character(parche) %>%
#       tolower(),
#     arrecife = as.character(arrecife) %>%
#       tolower(),
#     # pesos enteros para calcular medianas.
#     peso_entero = 10^9 * peso
#   ) %>%
#   # Calculando tabla por polígono
#   ddply(.(ID), function(x){
#     
#       # se nombran las variables porque se usará ldply y se quiere tener el .id
#       variables_stack <- c(
#         "WV2_2010_1" = "WV2_2010_1",
#         "WV2_2010_2" = "WV2_2010_2", # Muy correlacionada con WV2_2010_1
#         "WV2_2010_3" = "WV2_2010_3", # Muy correlacionada con WV2_2010_1
#         "WV2_2010_4" = "WV2_2010_4",
#         "batimetria" = "batimetria",
#         "pendiente" = "pendiente",
#         "aspecto" = "aspecto"
#       )
#       
#       # Calculando resúmenes por variable para cada polígono.
#       resumenes_poligono <- ldply(variables_stack, function(variable,x){
#         # Obteniendo los valores de cada variable de una manera un tanto rara,
#         # pues no se puede hacer x$variable pues "variable" es una variable
#         valores <- x[[variable]]
#         pesos <- x$peso_entero
#         
#         resumenes_variable <-data_frame(
#           media =  mean(valores, na.rm = TRUE),
#           media_ponderada = weighted.mean(valores, pesos, na.rm = TRUE),
#           mediana = median(valores, na.rm = TRUE),
#           mediana_ponderada = weightedMedian(valores, pesos, na.rm = TRUE),
#           sd = sd(valores, na.rm = TRUE),
#           sd_ponderada = weightedSd(valores, pesos, na.rm = TRUE)
#         )
#         return(resumenes_variable)
#       }, x) %>%
#         gather(resumen, valor, -.id) %>%
#         # Creando títulos
#         transmute(
#           titulo = paste0(.id, "_", resumen),
#           valor = valor
#         ) %>%
#         # Acomodando tabla
#         spread(titulo, valor)
#       
#       # Creando tabla final para el polígono correspondiente
#       tabla_poligono <- cbind(
#         data_frame(
#           punto = x$punto[1],
#           parche = x$parche[1],
#           arrecife = x$arrecife[1],
#           latitud = x$latitud[1],
#           longitud = x$longitud[1],
#           profundidad_media_m = x$profundidad_media_m[1],
#           cobertura_acropora = x$cobertura_acropora[1]
#         ),
#         resumenes_poligono
#       )
#     })
# saveRDS(df_trabajo_2010, "../../productos/df_trabajo_2010.RData")
df_trabajo_2010 <- readRDS("../../productos/df_trabajo_2010.RData")

# comprobando número de polígonos para el 2010.
nrow(df_trabajo_2010)

df_pixeles_desglosados %>%
  filter(!is.na(WV2_2010.12.20_Mexico_bref_geo_UTM16N.1)) %>%
  group_by(ID) %>%
  tally() %>%
  nrow()
# bien!

# Y el DF de trabajo para el 2011
# df_trabajo_2011 <- df_pixeles_desglosados %>%
#   filter(!is.na(WV2_2011.01.19_Mexico_bref_geo_UTM16N.1)) %>%
#   select(
#     ID,
#     WV2_2011_1 = WV2_2011.01.19_Mexico_bref_geo_UTM16N.1,
#     WV2_2011_2 = WV2_2011.01.19_Mexico_bref_geo_UTM16N.2,
#     WV2_2011_3 = WV2_2011.01.19_Mexico_bref_geo_UTM16N.3,
#     WV2_2011_4 = WV2_2011.01.19_Mexico_bref_geo_UTM16N.4,
#     batimetria = batimetria_real_pm_completa1,
#     pendiente = slope_pm1,
#     aspecto = aspect_pm,
#     peso = weight,
#     punto = Punto,
#     parche = Parche,
#     arrecife = Arrecife,
#     latitud = Lat,
#     longitud = Long,
#     profundidad_media_m = Av_depth_m,
#     cobertura_acropora = Acropora_c
#   ) %>%
#   mutate(
#     punto = as.character(punto) %>%
#       tolower(),
#     parche = as.character(parche) %>%
#       tolower(),
#     arrecife = as.character(arrecife) %>%
#       tolower(),
#     # pesos enteros para calcular medianas.
#     peso_entero = 10^9 * peso
#   ) %>%
#   # Calculando tabla por polígono
#   ddply(.(ID), function(x){
#     
#       # se nombran las variables porque se usará ldply y se quiere tener el .id
#       variables_stack <- c(
#         "WV2_2011_1" = "WV2_2011_1",
#         "WV2_2011_2" = "WV2_2011_2", # Muy correlacionada con WV2_2011_1
#         "WV2_2011_3" = "WV2_2011_3", # Muy correlacionada con WV2_2011_1
#         "WV2_2011_4" = "WV2_2011_4",
#         "batimetria" = "batimetria",
#         "pendiente" = "pendiente",
#         "aspecto" = "aspecto"
#       )
#       
#       # Calculando resúmenes por variable para cada polígono.
#       resumenes_poligono <- ldply(variables_stack, function(variable,x){
#         # Obteniendo los valores de cada variable de una manera un tanto rara,
#         # pues no se puede hacer x$variable pues "variable" es una variable
#         valores <- x[[variable]]
#         pesos <- x$peso_entero
#         
#         resumenes_variable <-data_frame(
#           media =  mean(valores, na.rm = TRUE),
#           media_ponderada = weighted.mean(valores, pesos, na.rm = TRUE),
#           mediana = median(valores, na.rm = TRUE),
#           mediana_ponderada = weightedMedian(valores, pesos, na.rm = TRUE),
#           sd = sd(valores, na.rm = TRUE),
#           sd_ponderada = weightedSd(valores, pesos, na.rm = TRUE)
#         )
#         
#         return(resumenes_variable)
#       }, x) %>%
#         gather(resumen, valor, -.id) %>%
#         # Creando títulos
#         transmute(
#           titulo = paste0(.id, "_", resumen),
#           valor = valor
#         ) %>%
#         # Acomodando tabla
#         spread(titulo, valor)
#       
#       # Creando tabla final para el polígono correspondiente
#       tabla_poligono <- cbind(
#         data_frame(
#           punto = x$punto[1],
#           parche = x$parche[1],
#           arrecife = x$arrecife[1],
#           latitud = x$latitud[1],
#           longitud = x$longitud[1],
#           profundidad_media_m = x$profundidad_media_m[1],
#           cobertura_acropora = x$cobertura_acropora[1]
#         ),
#         resumenes_poligono
#       )
#     })
# 
# # Comprobando número de polígonos totales
# nrow(df_trabajo_2011)
# 
# df_pixeles_desglosados %>%
#   filter(!is.na(WV2_2011.01.19_Mexico_bref_geo_UTM16N.1)) %>%
#   group_by(ID) %>%
#   tally() %>%
#   nrow()
# # bien!
# 
# # Y la cantidad de registros en la unión:
# nrow(df_trabajo_2010) + nrow(df_trabajo_2011) - nrow(df_trabajo_2010 %>%
#     inner_join(df_trabajo_2011, by = "ID"))
# 
# # Si usamos el df del 2011, sólo ganamos 45 nuevos puntos de 925, por lo tanto,
# # no vale la pena el esfuerzo
# nrow(df_trabajo_2011) - nrow(df_trabajo_2010 %>%
#     inner_join(df_trabajo_2011, by = "ID"))
# 
# df_pixeles_desglosados %>%
#   group_by(ID) %>%
#   tally() %>%
#   nrow()
# # perfecto! Se puede comenzar con los análisis
```

Seleccionando del df de trabajo variables de interés, con ayuda de un mapa de
calor de las correlaciones entre las variables.
```{r, warning=FALSE, message=FALSE}

# Calculando la correlación entre las variables de interés
correlaciones_variables <- df_trabajo_2010 %>%
  filter(complete.cases(.)) %>%
  select(
    latitud,
    longitud,
    profundidad_media_m,
    cobertura_acropora,
    aspecto_media:WV2_2010_4_sd_ponderada
  ) %>%
  cor() %>%
  as_data_frame() %>%
  mutate(
    variable_1 = colnames(.)
  ) %>%
  select(
    variable_1,
    everything()
  ) %>%
  gather(
    key = variable_2, value = correlacion, latitud:WV2_2010_4_sd_ponderada
  )

# Mapa de calor de las correlaciones
ggplot(correlaciones_variables,
  aes(x = variable_1, y = variable_2, fill = correlacion)) +
  geom_tile() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Podemos ver un patrón muy interesante. Sólo necesitamos quedarnos
# con lo siguiente:
# 1. Por variable, con una medida de tendencia central y una de dispersión.
# 2. Sólo una de las variables WV2_2010_1:WV2_2010_2:

df_trabajo_2010_variables_selectas <- df_trabajo_2010 %>%
  mutate(
    # Categorizando cobertura de acrópora por cuantiles
    cat_cobertura_acropora_2 = cut(cobertura_acropora,
      breaks = quantile(cobertura_acropora, probs = c(0, 0.5, 1)),
      include.lowest = TRUE),
    
    cat_cobertura_acropora_4 = cut(cobertura_acropora,
      breaks = quantile(cobertura_acropora, probs = seq(0, 1, 0.25)),
      include.lowest = TRUE),
    
    # Para validación cruzada espacial
    parche = as.factor(parche),
    arrecife = as.factor(arrecife),
    
    cat_latitud_4 = cut(latitud,
      breaks = quantile(latitud, probs = seq(0, 1, 0.25)),
      include.lowest = TRUE),

    cat_latitud_5 = cut(latitud,
      breaks = quantile(latitud, probs = seq(0, 1, 0.2)),
      include.lowest = TRUE),

    cat_latitud_10 = cut(latitud,
      breaks = quantile(latitud, probs = seq(0, 1, 0.1)),
      include.lowest = TRUE)
    
  ) %>%
  # Para plotear arrecifes en orden de latitud
  group_by(arrecife) %>%
    mutate(
      latitud_media_arrecife = mean(latitud)
    ) %>%
  ungroup() %>%
  select(
    ID,
    
    # VC espacial
    parche,
    arrecife,
    latitud_media_arrecife,
    cat_latitud_4,
    cat_latitud_5,
    cat_latitud_10,
    
    # Variables de salida
    cobertura_acropora,
    cat_cobertura_acropora_2,
    cat_cobertura_acropora_4,
    
    # Variables de entrada
    latitud,
    longitud,
    profundidad_media_m,
    dplyr::contains("media_ponderada"),
    dplyr::contains("sd_ponderada"),
    -dplyr::contains("WV2_2010_2"),
    -dplyr::contains("WV2_2010_3")
  ) %>%
  filter(complete.cases(.))
```

Análisis exploratiorio de datos:

```{r, warning=FALSE, message=FALSE}
# Análisis exploratorio

df_plot_2010 <- df_trabajo_2010_variables_selectas %>%
  gather(llave, valor, latitud:WV2_2010_4_sd_ponderada)
glimpse(df_plot_2010)

# Primero que nada, haciendo el histograma de cobertura de acrópora para ver si
# es continua o está "implícitamente categorizada" por tendencias a redondear los
# datos de campo.
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = cobertura_acropora)) +
  geom_histogram()
# Claramente se ve el efecto por redondeo, tal vez salga mejor categorizando.

# Cantidad de registros por arrecife:
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = reorder(arrecife, latitud_media_arrecife))) +
  geom_bar() +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
    )

#  Ploteando los sitios por arrecife:
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = longitud, y = latitud, colour = arrecife)) +
  geom_point()

# Perfilando las variables ambientales por arrecife:
ggplot(data = df_plot_2010,
  aes(x = reorder(arrecife, latitud_media_arrecife), y = valor)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, size = 0.5) +
  facet_wrap(~llave, scales = "free", ncol = 5) +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
    )
# Se ve que Limones tiene densidades únicas en varias variables. Por ejemplo,
# WV2_2010_1/4_media_ponderada o WV2_2010_1_sd_ponderada. Esto es importante
# porque puede justificar el hecho que cuando no se entrena con datos de limones,
# sube mucho el error de predicción para este arrecife, y apoya que nos quedemos con
# una VC que imponga

# Perfilando la cobertura por arrecife:
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = reorder(arrecife, latitud_media_arrecife), y = cobertura_acropora)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, size = 0.5) +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
    )

# Perfilando las categorías de cobertura por arrecife cat 4:
ggplot(data = df_trabajo_2010_variables_selectas %>%
    # Quitando arrecifes con muy pocas observaciones.
    group_by(arrecife) %>%
    mutate(cuenta = n()) %>%
    ungroup() %>%
    filter(cuenta > 50),
  aes(x = reorder(arrecife, latitud_media_arrecife),
    fill = cat_cobertura_acropora_4)) +
  geom_bar(position = position_fill(reverse = TRUE)) +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
    )
# Limones tiene mayor proporción de sitios con cobertura entre 50 y 100,
# y menor proporción con cobertura de [0, 15].
# Puerto Morelos tiene mayor proporción con cobertura de [0, 15] y ninguno
# con proporción de (50,100].
# Se ve una ligera  tendencia latitudinal: a menor latitud, menor cobertura

# Perfilando las categorías de cobertura por arrecife cat 2:
ggplot(data = df_trabajo_2010_variables_selectas %>%
    # Quitando arrecifes con muy pocas observaciones.
    group_by(arrecife) %>%
    mutate(cuenta = n()) %>%
    ungroup() %>%
    filter(cuenta > 50),
  aes(x = reorder(arrecife, latitud_media_arrecife),
    fill = cat_cobertura_acropora_2)) +
  geom_bar(position = position_fill(reverse = TRUE)) +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
    )
# La tendencia latitudinal es clara ahora...

# Ploteando cobertura continua en el mapa
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = longitud, y = latitud, colour = cobertura_acropora)) +
  geom_point(alpha = 0.3) +
  scale_color_gradientn(colours = rainbow(5))
# Parece confetti, se espera que ni la latitud y longitud solas expliquen bien la
# cobertura de A. palmata

# Ploteando cobertura discretizada a 2 clases en el mapa
ggplot(data = df_trabajo_2010_variables_selectas, aes(x = longitud, y = latitud, colour = cat_cobertura_acropora_2)) +
  geom_point(alpha = 0.3)
# Se ve ligeramente la tendencia latitudinal

# Diagramas de dispersión de varias variables contra cobertura de acrópora
ggplot(data = df_plot_2010, aes(x = valor, y = cobertura_acropora)) +
  geom_point(alpha = 0.01) +
  geom_smooth() +
  facet_wrap(~llave, scales = "free", ncol = 2) +
  ylim(0, 70)
# Las tendencias son muy suaves, casi nulas, para las variables que puedieran
# presentarlas.

ggplot(data = df_plot_2010, aes(x = cobertura_acropora, y = valor)) +
  geom_point(alpha = 0.01) +
  geom_smooth() +
  facet_wrap(~llave, scales = "free", ncol = 2)
# Aquí se aprecia mejor la tendencia en latitud y longitud, si en aspecto
# hay algo, es muy débil para notarlo. Es decir, latitud y longitud son nuestras
# mejores variables.

# Ahora perfilando las distintas variables por la categorización de la cobertura
# de acrópora:
ggplot(data = df_plot_2010, aes(x = cat_cobertura_acropora_2, y = valor)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.03) +
  facet_wrap(~llave, scales = "free", ncol = 2)
# Se ven tendencias en "latitud", "longitud" y "aspecto_media_ponderada"

ggplot(data = df_plot_2010, aes(x = cat_cobertura_acropora_4, y = valor)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.03) +
  facet_wrap(~llave, scales = "free", ncol = 2)
# Se ven tendencias en "latitud", "longitud" y "aspecto_media_ponderada"


df_plot_cat_latitud <- df_trabajo_2010_variables_selectas %>%
  mutate(
    cat_latitud_4 = as.character(cat_latitud_4),
    cat_latitud_5 = as.character(cat_latitud_5),
    cat_latitud_10 = as.character(cat_latitud_10)
  ) %>%
  gather(cat_latitud, intervalo, cat_latitud_4, cat_latitud_5, cat_latitud_10) %>%
  # Calculando latitud media por intervalo para plotearlos en orden
  group_by(intervalo) %>%
  mutate(
    latitud_media_intervalo = mean(latitud)
  ) %>%
  ungroup()

# Graficando las categorías de latitud en el mapa:
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = longitud, y = latitud, colour = cat_latitud_4)) +
  geom_point()

ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = longitud, y = latitud, colour = cat_latitud_5)) +
  geom_point()

ggplot(data = df_trabajo_2010_variables_selectas, #%>%
    #group_by(cat_latitud_10) %>%
    # revolviendo los colores
    #mutate(orden = sample(1:1000, 1)) %>%
    #ungroup(),
  aes(
    x = longitud,
    y = latitud,
    colour = cat_latitud_10)) + #reorder(cat_latitud_10, orden))) +
  geom_point()

# Ahora, perfilando cobertura de acrópora por categorías de latitud.
ggplot(data = df_plot_cat_latitud,
  aes(x = reorder(intervalo, latitud_media_intervalo), y = cobertura_acropora)) +
  geom_boxplot() +
  facet_wrap(~cat_latitud, scales = "free") +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
  )
# Con cat_latitud 10 se rompe la tendencia longitudinal, veamos ahora con las categorías
# de cobertura de acrópora.

ggplot(data = df_plot_cat_latitud,
  aes(x = reorder(intervalo, latitud_media_intervalo), fill = cat_cobertura_acropora_4)) +
  geom_bar(position = position_fill(reverse = TRUE)) +
  facet_wrap(~cat_latitud, scales = "free") +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
  )
# La tendencia latitudinal se observa con más claridad para menos folds que para más folds.

ggplot(data = df_plot_cat_latitud,
  aes(x = reorder(intervalo, latitud_media_intervalo), fill = cat_cobertura_acropora_2)) +
  geom_bar(position = position_fill(reverse = TRUE)) +
  facet_wrap(~cat_latitud, scales = "free") +
  theme(axis.text.x = element_text(
    size = 5,
    angle = 90,
    hjust = 1)
  )
# Se sigue observando la tendencia longitudinal, con el mismo patrón que el anterior.
# Sin embargo, para 10 categorías, se rompe esta tendencia en los folds 5 y 9 principal
# mente, es conveniente ver la predicción en estos, con el fin de evaluar si efectivamente
# se eestán usando las variables satelitales per se para predicción, y no simplemente
# como un proxy de la longitud.

# Finalmente, perfilando variables ambientales por categoría de latitud:

```

Intentos fallidos de clustering para tratar de ver si las unidades de muestreo
de acrópora (interesección) tiene distintas características en las variables de PR
de acuerdo a su localización geográfica, para ver si la VC no es injusta espacialmente.

```{r, warning=FALSE, message=FALSE}
# Clustering de variables ambientales, para ver si una validación cruzada espacial
# puede ser "injusta" con el modelo en ciertos folds (al no darle los insumos
# adecuados)

# df_trabajo_2010_kmeans <- df_trabajo_2010_variables_selectas %>%
#   select(
#     aspecto_media_ponderada:WV2_2010_4_sd_ponderada
#   ) %>%
#   # Para estandarizar datos
#   base::scale() %>%
#   as_data_frame()
# 
# # K-medias
# set.seed(12345)
# 
# # Haciendo la gráfica de codo para obtener el número óptimo de clusters:
# df_grafica_codo <- ldply(1:500, function(i){
#   clustering <- kmeans(df_trabajo_2010_kmeans,
#     centers = i,
#     iter.max = 50,
#     nstart = 25)
#   resultado <- data_frame(
#     num_clusters = i,
#     withinss = clustering$tot.withinss
#     )
#   return(resultado)
# })
# 
# ggplot(data = df_grafica_codo, aes(x = num_clusters, y = withinss)) +
#   geom_line()
# # Esto va a ser un análisis en sí mismo porque la gráfica de codo no me dice mucho.

# matriz_distancias <- df_trabajo_2010_variables_selectas %>%
#   select(
#     aspecto_media_ponderada:WV2_2010_4_sd_ponderada
#   ) %>%
#   # Para estandarizar datos
#   base::scale() %>%
#   dist()

# Creo que clustering Jerárquico ni T-SNE tienen sentido porque
# se basan en distancias entre sitios, y eso creo que es raro considerando la naturaleza de las variables.

# # Clustering Jerárquico. Como con vecino más lejano me salía un cluster muy grande,
# # mejor uso Ward para minimizar within SST
# clustering <- hclust(matriz_distancias, method = "ward.D")
# plot(clustering, hang = -1)
# 
# df_trabajo_2010_grupos_hclust <- df_trabajo_2010_variables_selectas %>%
#   mutate(
#     grupo =  cutree(clustering, h = 25) %>%
#       as.character()
#   )
# 
# df_trabajo_2010_grupos_hclust %>%
#   group_by(grupo) %>%
#   tally()
# 
# ggplot(data = df_trabajo_2010_grupos_hclust,
#   aes(x = longitud,
#     y = latitud,
#     colour = grupo)) +
#   geom_point(alpha = 0.5) +
#   facet_grid(~grupo)
# Jajaja no sale nada
  
# # T-SNE
# tsne_variables_ambientales <- tsne(matriz_distancias, perplexity = 100, max_iter = 2000) %>%
#   as_data_frame() %>%
#   mutate(
#     arrecife = df_trabajo_2010_variables_selectas$arrecife
#   )
# 
# ggplot(tsne_variables_ambientales,
#   aes(x = V1, y = V2, colour = arrecife)) +
#   geom_point()
# # T-SNE no me sirve porque quiero distinguir entre si un arrecife está lejos
# # de otros, no cerca.
```

Ajuste preeliminar de modelos predictivos, ésto se hace sólo desde el punto de vista
exploratorio, pues no se está controlando por autocorrelación espacial.

```{r, warning=FALSE, message=FALSE}
# Ajuste de primeros modelos predictivos:
# 1. Random Forest, sin tomar en cuenta autocorrelación espacial.
# 2. Usando todas las variables, y luego quitando latitud y longitud
# 3. Usando como variable de salido: "cobertura_acropora"" (regresión),
# "cat_cobertura_acropora_2" y "cat_cobertura_acropora_4" (clasificación).

set.seed(12345)

# En "df_trabajo_2010_variables_selectas" ya se filtraron registros con NA's
y_regresion <- df_trabajo_2010_variables_selectas %>%
  '$'(., cobertura_acropora)

y_clasificacion_2 <- df_trabajo_2010_variables_selectas %>%
  '$'(., cat_cobertura_acropora_2)

y_clasificacion_4 <- df_trabajo_2010_variables_selectas %>%
  '$'(., cat_cobertura_acropora_4)

# Primer round: todas las variables vs "y_regresion", "y_clasificacion_2" y
# "y_clasificacion_4"

x1 <- df_trabajo_2010_variables_selectas %>%
  select(
    latitud:WV2_2010_4_sd_ponderada
  )
glimpse(x1)

# No explica mucho para regresión
rf_1_regresion <- randomForest(x = x1, y = y_regresion, importance = TRUE)
  
rf_1_clasificacion_2 <- randomForest(x = x1, y = y_clasificacion_2,
  importance = TRUE)
# Tasa de éxito del ~65%, mucho mejor.

varImpPlot(rf_1_clasificacion_2)
# Longitud y latitud son las más importantes, hay que ver qué onda si hacemos
# validación cruzada espacial.

rf_1_clasificacion_4 <- randomForest(x = x1, y = y_clasificacion_4,
  importance = TRUE)
# Tasa de éxito del ~45%, por lo que funciona mejor para 2 clases.

# Seleccionando otras variables:

# x1 sin latitud ni longitud
x2 <- df_trabajo_2010_variables_selectas %>%
  select(
    profundidad_media_m:WV2_2010_4_sd_ponderada
  )
glimpse(x2)

rf_2_clasificacion_2 <- randomForest(x = x2, y = y_clasificacion_2,
  importance = TRUE)
varImpPlot(rf_2_clasificacion_2)
# Tasa de éxito de ~63%, no baja mucho!

# obteniendo las variables más importantes en varias corridas del rf anterior:
df_variables_importantes <- ldply(1:100, function(i, x2, y_clasificacion_2){
  # Ajustando un Random Forest:
  rf_aux <- randomForest(x = x2, y = y_clasificacion_2, importance = TRUE)
  
  # Calculando las importancias de las variables:
  importancia_variables_aux <- varImpPlot(rf_aux) %>%
    as.matrix()
  nombres_variables <- rownames(importancia_variables_aux)
  
  # Generando data frame con las 5 variables más importantes, en órden, por tipo
  # de prueba:
  variables_importantes_exactitud_df <- importancia_variables_aux %>%
    as_data_frame() %>%
    mutate(
      variable = nombres_variables
    ) %>%
    arrange(desc(MeanDecreaseAccuracy)) %>%
    head(5) %>%
    transmute(
      posicion_exactitud = 1:nrow(.),
      variable_exactitud = variable
    )
  
    variables_importantes_gini_df <- importancia_variables_aux %>%
    as_data_frame() %>%
    mutate(
      variable = nombres_variables
    ) %>%
    arrange(desc(MeanDecreaseGini)) %>%
    head(5) %>%
    transmute(
      posicion_gini = 1:nrow(.),
      variable_gini = variable
    )
    
    df_resultado <- variables_importantes_exactitud_df %>%
      cbind(variables_importantes_gini_df) %>%
      mutate(
        corrida = i
      ) %>%
      select(
        corrida,
        everything()
      )

    return(df_resultado)
  
}, x2, y_clasificacion_2)

variables_importantes_gini <- df_variables_importantes %>%
  group_by(posicion_gini, variable_gini) %>%
  tally() %>%
  arrange(posicion_gini, desc(n))

variables_importantes_exactitud <- df_variables_importantes %>%
  group_by(posicion_exactitud, variable_exactitud) %>%
  tally() %>%
  arrange(posicion_exactitud, desc(n))

# Utilizando las variables más importantes de acuerdo a varias corridas del RF anterior:

# - WV2_2010_1_sd_ponderada
# - WV2_2010_4_sd_ponderada
# - aspecto_media_ponderada
# - pendiente_sd_ponderada
# - batimetria_media_ponderada
# - WV2_2010_4_media_ponderada

x3 <- df_trabajo_2010_variables_selectas %>%
  select(
    WV2_2010_1_sd_ponderada,
    WV2_2010_4_sd_ponderada,
    aspecto_media_ponderada,
    pendiente_sd_ponderada,
    batimetria_media_ponderada,
    WV2_2010_4_media_ponderada
  )
glimpse(x3)

rf_3_clasificacion_2 <- randomForest(x = x3, y = y_clasificacion_2,
  importance = TRUE)
varImpPlot(rf_3_clasificacion_2)
# Tasa de éxito ~62.5%. Nos quedamos con esas variables, por ahora.
```

Ahora sí, ajustando y probando los modelos mediante validación cruzada espacial
k = 4, 5, 10 (número de folds)

Obteniendo proporciones de éxito tanto a nivel de fold (segmento del espacio),
para analizar la situación particular de cada segmento espacial estudiado, como
a nivel de iteración, para comparar con un modelo de etiquetado al azar.

Se calculan también ayudas visuales como la tabla de número de puntos de cada
arrecife por fold, y mapas de los folds y arrecifes.

```{r, warning=FALSE, message=FALSE}
## Validación cruzada espacial

# # Explorando las variables naturales que podrían ser factores para la VC espacial
# df_trabajo_2010_variables_selectas %>%
#   group_by(parche) %>%
#   tally() %>%
#   arrange(desc(n)) %>%
#   View()
# # Son muy poco homogéneos los parches en cantidad de puntos, por ello, se debe
# # hacer el promedio _ponderado_ de los errores de CV para calcular el error CV.
# # Pero esto es un problema que puede tender a inflar el error de CV: suponer que
# # se tienen 100 datos y dos particiones, de 90 y 10 datos respectivamente.
# # Si hacemos lo anterior, entrenamos con 90, predecimos con 10 y esta estimación
# # con un modelo "bien" ajustado pesa poco, pero la contraria: entrenar con 10 y
# # predecir con 90 es una estimación con un modelo mal ajustado que pesa mucho.
# 
# df_trabajo_2010_variables_selectas %>%
#   group_by(arrecife) %>%
#   tally() %>%
#   arrange(desc(n)) %>%
#   View()
# # También muy poco uniformes.

# Haciendo 10000 repeticiones con de la VC, para evaluar si hay evidencia de que
# corrige autocorrelación espacial. Si así es, se espera que la densidad del
# error en cada fold espacial sea parecida.

# Si un fold resulta muy bien predicho, y esto coincide con un fold al que
# pertenecen no todos los datos de un arrecife, hay algo raro.

# Esto NO es cierto, lo que estas modas pueden indicar es la presencia de agregados
# (arrecifes u otro nivel), cuyas cobertura de acrópora palmata sigue patrones
# similares ante distintas variables captadas mediante percepción remota
# (puesto que autocorrelación espacial se está controlando a nivel, por ejemplo,
# de Limones al segmentarlo en distintos folds).

# Que haya folds muy mal predichos podría deberse a que es necesario hacer
# el análisis por agregados para mejorar la predicción.

# Esto sugiere un siguiente paso para este análisis: encontrar dichos agregados
# (Limones parece ser un caso promisorio), predecir la cobertura de coral mediante
# variables satelitales, evaluar el desempeño de este método en distintas fechas
# y con datos de campo apropiados, Si la predicción es promisoria, y las variables
# importantes se mantienen, se podría ir pensando en un método de homologación de
# imágenes satelitales para predecir con ellas y tener una manera de monitorear
# el arrecife reduciendo la cantidad de trabajo de campo realizado.

# Es una buena noticia encontrar estos agregados, porque quiere decir que la
# predicción se puede mejorar mucho a esos niveles, que la predicción global que
# se presenta en este artículo.

# Recordar la función: "validacion_cruzada_por_segmentos"

# Creando la fórmula:
formula_prediccion <- paste0("cat_cobertura_acropora_2 ~ ",
  colnames(
    df_trabajo_2010_variables_selectas %>%
      select(
        aspecto_media_ponderada:WV2_2010_4_sd_ponderada
        )
  ) %>%
  paste0(collapse = " + ")) %>%
  as.formula()

num_iteraciones <- 10000

### Corriendo la VC espacial para 4/5/10 clases de latitud

# cv_cat_latitud_4 <- validacion_cruzada_por_segmentos(
#   df_trabajo_2010_variables_selectas,
#   "cat_latitud_4",
#   formula_prediccion,
#   num_iteraciones)
# saveRDS(cv_cat_latitud_4, "../../productos/cv_cat_latitud_4.RData")

cv_cat_latitud_4 <- readRDS("../../productos/cv_cat_latitud_4.RData") %>%
  mutate(
    segmento = as.factor(segmento) %>%
      # redefiniendo el primer elemento en los factores.
      relevel("[20.86,20.88]")
  )

# cv_cat_latitud_5 <- validacion_cruzada_por_segmentos(
#   df_trabajo_2010_variables_selectas,
#   "cat_latitud_5",
#   formula_prediccion,
#   num_iteraciones)
# saveRDS(cv_cat_latitud_5, "../../productos/cv_cat_latitud_5.RData")

cv_cat_latitud_5 <- readRDS("../../productos/cv_cat_latitud_5.RData") %>%
  mutate(
    segmento = as.factor(segmento) %>%
      # redefiniendo el primer elemento en los factores.
      relevel("[20.855,20.874]")
  )

# cv_cat_latitud_10 <- validacion_cruzada_por_segmentos(
#   df_trabajo_2010_variables_selectas,
#   "cat_latitud_10",
#   formula_prediccion,
#   num_iteraciones)
# saveRDS(cv_cat_latitud_10, "../../productos/cv_cat_latitud_10.RData")

cv_cat_latitud_10 <- readRDS("../../productos/cv_cat_latitud_10.RData") %>%
  mutate(
    segmento = as.factor(segmento) %>%
      # redefiniendo el primer elemento en los factores.
      relevel("[20.855,20.869]")
  )

# Calculando la proporción de éxito de VC:
exito_vc_cat_latitud_4 <- weighted.mean(cv_cat_latitud_4$exito_prediccion,
  cv_cat_latitud_4$n) #0.5193142
exito_vc_cat_latitud_5 <- weighted.mean(cv_cat_latitud_5$exito_prediccion,
  cv_cat_latitud_5$n) #0.5488043
exito_vc_cat_latitud_10 <- weighted.mean(cv_cat_latitud_10$exito_prediccion,
  cv_cat_latitud_10$n) #0.576338

# Graficando las proporciones de exito por fold:
ggplot(cv_cat_latitud_4,
  aes(x = exito_prediccion, fill = segmento, group = segmento)) +
  geom_histogram(position = "identity", alpha = 0.8)
ggplot(cv_cat_latitud_5,
  aes(x = exito_prediccion, fill = segmento, group = segmento)) +
  geom_histogram(position = "identity", alpha = 0.8)
ggplot(cv_cat_latitud_10,
  aes(x = exito_prediccion, fill = segmento, group = segmento)) +
  geom_histogram(position = "identity", alpha = 0.8)

# Obteniendo las proporciones medias de éxito por fold
cv_cat_latitud_4 %>%
  group_by(segmento) %>%
  summarise(
    exito_prediccion_promedio = mean(exito_prediccion)
  )
cv_cat_latitud_5 %>%
  group_by(segmento) %>%
  summarise(
    exito_prediccion_promedio = mean(exito_prediccion)
  )
cv_cat_latitud_10 %>%
  group_by(segmento) %>%
  summarise(
    exito_prediccion_promedio = mean(exito_prediccion)
  )

#  Ploteando los sitios por arrecife:
ggplot(data = df_trabajo_2010_variables_selectas,
  aes(x = longitud, y = latitud, colour = arrecife)) +
  geom_point()

# Graficando los folds:
ggplot(df_trabajo_2010_variables_selectas,
  aes(
    x = longitud,
    y = latitud,
    colour = cat_latitud_4
  )) +
  geom_point()
ggplot(df_trabajo_2010_variables_selectas,
  aes(
    x = longitud,
    y = latitud,
    colour = cat_latitud_5
  )) +
  geom_point()
ggplot(df_trabajo_2010_variables_selectas,
  aes(
    x = longitud,
    y = latitud,
    colour = cat_latitud_10
  )) +
  geom_point()

# Tabla con número de puntos por categoría de latitud y arrecife
df_trabajo_2010_variables_selectas %>%
  group_by(cat_latitud_4, arrecife) %>%
  tally() %>%
  arrange(cat_latitud_4, arrecife)
df_trabajo_2010_variables_selectas %>%
  group_by(cat_latitud_5, arrecife) %>%
  tally() %>%
  arrange(cat_latitud_5, arrecife)
df_trabajo_2010_variables_selectas %>%
  group_by(cat_latitud_10, arrecife) %>%
  tally() %>%
  arrange(cat_latitud_10, arrecife)

# Intervalos de 5%:
# |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
# |-1-|-2-|-3-|-4-|-5-|-6-|-7-|-8-|-9-|-0-| (deciles)
# |----A----|----B----|----C----|----D----| (cuartiles)

# Al comparar como desciende la capacidad predictiva para el primer fold, al pasar de 4 a 5 folds 
# de VC parece que el asentamiento de las poblaciones de acrópora en cierta sección de Bocana y
# Picudas difiere de otra de Bocana y Puerto Morelos, por lo que convendría analizar estos dos
# estratos de manera distinta. Además, comparando con CV 10-fold, que vuelve a subir al incorporar
# más información de la parte Sur de Bocana en el set de entrenamiento, refuerza la evidencia a favor
# del uso de estratos para mejorar el poder predictivo de un modelo.

# Con respecto a Limones, se tiene evidencia de que puede constituir otro estrato, pues existe una
# diferencia entre predecir sobre él con 4 a 5 fold (sólo 5% de diferencia en cantidad de datos),
# pero este 5% son datos de Limones sólamente. Además, al comparar con el 10-fold, vemos que la sección
# de latitud media de Limones, que presenta menor cobertura que los extremos, es predicha igual de
# bien que las otras, lo que sugiere que las variables satelitales no están siendo utilizadas
# simplemente como un proxy de un gradiente latitudinal, sino que son predictoras por sí mismas.
# Además, las variables de PR en Limones alcanzan rangos que difícilmente se ven en otros arrecifes.

# Con respecto a los 10-folds 3, 4 y 5, que prácticamente coinciden con el segundo 4-fold, se ve que
# el éxito de predicción no cambia mucho si los incorporamos o no, lo que sugiere que los arrecifes
# de norte de Bocada/Picudas, Punta Caracol/Tanchacté y Cuevones se podrían considerar como estratos distintos. Habría que investigar si no es necesaria mayor estratificación.
# considerar como estratos distintos los arrecifes de picudas

# Para los otros segmentos (arrecifes herradura a manchones) la evidencia no es tan clara, sin embargo,
# la técnica de predicción por estratos parece promisoria y merecedora de estudios futuros.
```

Graficando éxito de predicción de 4/5/10-validación cruzada espacial contra un
modelo de predicción al azar.

```{r, warning=FALSE, message=FALSE}
# Prueba rápida con un modelo al azar:
# nrow(df_trabajo_2010_variables_selectas) # 876
# Simulemos la distribución del ECM al correr 1000 simulaciones de un etiquetado
# al azar de 876 registros (cantidad clasificada con Validación Cruzada)
# Sabemos que el modelo al azar sería la opción trivial más acertada puesto que
# hay la misma cantidad de registros con en cada categoría (por lo que no se puede,
# usar la moda, a diferencia si, por ejemplo, una categoría tuviera 90 registros y
# otra 10.

# Calculando las proporciónes de éxito por iteración, para graficarlas 
# contra una simulación de un clasificador aleatorio
exito_iteracion_cat_latitud_4 <- cv_cat_latitud_4 %>%
  group_by(num_iteracion) %>%
  summarise(
    exito = weighted.mean(exito_prediccion, n)
  )
exito_iteracion_cat_latitud_5 <- cv_cat_latitud_5 %>%
  group_by(num_iteracion) %>%
  summarise(
    exito = weighted.mean(exito_prediccion, n)
  )
exito_iteracion_cat_latitud_10 <- cv_cat_latitud_10 %>%
  group_by(num_iteracion) %>%
  summarise(
    exito = weighted.mean(exito_prediccion, n)
  )

simulacion_error_azar <- ldply(1:10000, function(i){
  # TRUE: bien clasificado, FALSE: mal clasificado.
  muestra <- sample(x = c(TRUE, FALSE),
    size = nrow(df_trabajo_2010_variables_selectas),
    replace = TRUE,
    prob = c(0.5, 0.5))
  
  resultado <- data_frame(
    i = i,
    clasificacion_correcta = muestra
    )
  return(resultado)
  }) %>%
  group_by(i) %>%
  summarise(
    porcentaje_exito = sum(clasificacion_correcta)/n()
  ) %>%
  ungroup()

plot_distribuciones_aleatorio_rf <- simulacion_error_azar %>%
  mutate(
    tipo = "Clasificación al azar"
  ) %>%
  union(
    exito_iteracion_cat_latitud_4 %>%
      transmute(
        i = num_iteracion,
        porcentaje_exito = exito,
        tipo = "RF variables satelitales 4-fold CV"
      )
  ) %>%
  union(
    exito_iteracion_cat_latitud_5 %>%
      transmute(
        i = num_iteracion,
        porcentaje_exito = exito,
        tipo = "RF variables satelitales 5-fold CV"
      )
    ) %>%
    union(
    exito_iteracion_cat_latitud_10 %>%
      transmute(
        i = num_iteracion,
        porcentaje_exito = exito,
        tipo = "RF variables satelitales 10-fold CV"
      )
    )


ggplot(plot_distribuciones_aleatorio_rf,
  aes(x = porcentaje_exito, fill = tipo, group = tipo)) +
  geom_histogram(position="identity", alpha = 0.6)

ggplot(plot_distribuciones_aleatorio_rf,
  aes(x = tipo, y = porcentaje_exito)) +
  geom_boxplot()

# Calculando p-values de la media de cada distribución
# con respecto a "Clasificación al azar"
#percentil_5_cat_latitud_5 <- quantile(exito_iteracion_cat_latitud_5$exito, probs = 0.05)
p_value_cat_latitud_4 <- simulacion_error_azar %>%
  filter_(paste0("porcentaje_exito >= ", mean(exito_iteracion_cat_latitud_4$exito))) %>%
  summarise(
    p_value = n()/nrow(simulacion_error_azar)
  ) %>%
  '$'(., p_value)

p_value_cat_latitud_5 <- simulacion_error_azar %>%
  filter_(paste0("porcentaje_exito >= ", mean(exito_iteracion_cat_latitud_5$exito))) %>%
  summarise(
    p_value = n()/nrow(simulacion_error_azar)
  ) %>%
  '$'(., p_value)

p_value_cat_latitud_10 <- simulacion_error_azar %>%
  filter_(paste0("porcentaje_exito >= ", mean(exito_iteracion_cat_latitud_10$exito))) %>%
  summarise(
    p_value = n()/nrow(simulacion_error_azar)
  ) %>%
  '$'(., p_value)
```

Revisando la importancia de variables de acuerdo a la validación cruzada 5-folds

```{r, warning=FALSE, message=FALSE}
# cv_k_folds_cat_latitud_5_importancia_variables <- parsperrorest(
#   formula,
#   data = df_trabajo_2010_variables_selectas,
#   coords = c("longitud", "latitud"),
#   model.fun = randomForest,
#   pred.fun = predict,
#   smp.fun = partition.factor.cv,
#   smp.args = list(
#     fac = "cat_latitud_5",
#     repetition = 1:4,
#     nfold = 5,
#     seed1 = 123),
#   par.args = list(
#     par.units = 4,
#     par.mode = 1),
#   error.fold = TRUE,
#   error.rep = TRUE,
#   err.train = TRUE, # por curiosidad
#   benchmark = TRUE,
#   importance = TRUE, # muy tardada, pero útil.
#   imp.permutations = 200,
#   progress = 1)
# saveRDS(cv_k_folds_cat_latitud_5_importancia_variables,
# "../../productos/cv_k_folds_cat_latitud_5_importancia_variables.RData")
cv_k_folds_cat_latitud_5_importancia_variables <-
  readRDS("../../productos/cv_k_folds_cat_latitud_5_importancia_variables.RData")

# Viendo las variables más importantes:
summary(cv_k_folds_cat_latitud_5_importancia_variables$importance) %>%
  mutate(
    variables = row.names(.)
  ) %>%
  select(
    variables,
    everything()
  ) %>%
  arrange(desc(mean.error)) %>%
  select(
    variables,
    mean.error
  )
```

Conclusiones:
- Limones se comporta "diferente" a los otros arrecifes (ver 4-fold vs 5/10 fold),
- Sin embargo, aún controlando por autocorrelación espacial, Limones posiblemente pueda considerarse como un "agregado" cuyas cobertura de acrópora se pueda estimar
razonablemente bien mediante imágenes satelitales, al hacer un análisis específico
para él.
- Se puede buscar la existencia de otros agregados.
- Si todo sale bien (se encuentran agregados con la misma relación entre variables de percepción remota y cobertura de acrópora, al controlar la autocorrelación espacial), se podría entrenar un modelo para cada uno de ellos en determinada fecha, validarlo sobre distintos fechas con datos de campo (homologando variables satelitales), y si pasa las validaciones, usarlo para predecir cobertura de
acrópora mediante imágenes satelitales.
- Controlar por autocorrelación espacial es muy necesario para la generalización
de los modelos (si no lo hacemos, quién nos asegura que nuestro modelo predice
cobertura muy bien porque "delineó" los cuadritos existentes de cobertura, ¿y en
realidad está usando la información espacial para estimar latitud y longitud?)

Pasos futuros a considerar:
- Considerar el aspecto como una variable circular 0º = 360º, en especial para
 calcular su media y desviación estándar, que se ve que es una variable potencialmente significativa.
 - Usar features para hacer deep learning (ajuste de redes neuronales convolucionadas).
 - Hablar con expertos para que probar otras variables remotas potencialmente útiles
 para la predicción de A. palmata.
 - Hacer un análisis a nivel de parche, par tener mayor libertad de variables a elegir
 (cuantiles de las variables de percepción remota), así como para minimizar el efecto
 del error de GPS.
 - Existe error de GPS: la pocisión muestreada no empata siempre con la registrada
 exactamente, por ello, mediante VC se podría elegir el radio óptimo del buffer
 que optimice dos fuerzas opuestas: exactitud (que el buffer contenga en buena
 medida el sitio verdadero), y precisión (que el buffer sea lo más pequeño posible).
 - Se podría entrenar el RandomForest dividiendo los sitios en estratos espaciales
 y tomando una muestra de cada estrato, con el fin de mejorar la extrapolación
 espacial del modelo, al disminuir autocorrelación espacial (notar que muy probablemente
 se está sobreajustando el modelo, y aunque aún así se valida que funciona mejor
 que elección al azar para predicciones fuera del rango espacial de entrenamiento,
 se espera una notable mejora en el poder predictivo al ejercer esta correcció
 n).
 Se podría escoger una buena estratificación mediante validación cruzada.
 
Presentación:
1. Método
2. Histogramas de éxito de predicción vs error aleatorio.
3. Presentación del análisis por fold, conclusiones sobre estratificación y nos
quedamos con el mejor modelo 5-fold, descartando 4 fold por variables ambientales
en Limones.
4. Presentación de p-value e importancia de variables.

Sergio:
1. Me ofrece los viáticos del Mau.
2. Por experiencia del Mau, me va a estar fregando por eso para siempre. Es un anzuelo.
3. Mejor le digo a Lorenzo que si me consigue para el avión y lo que sea para la comida ya la armamos.